## Data Poisoning & Backdoor Attacks
This repository contains my implementation and experimentation on data poisoning and backdoor attacks in machine learning models.
The work was developed as part of my coursework on machine learning security at Georgia Tech, and reflects my personal exploration of adversarial vulnerabilities in modern ML systems.

## Intent
The objective of this project is to study:
1. How training data poisoning can implant hidden behaviors in models
2. How such backdoors affect model reliability and trustworthiness
3. The broader security implications for machine learning pipelines

## Ethical Use Statement
This project is strictly for educational and research purposes.
The code is not intended for:
1. Attacking real-world systems
2. Causing harm or data manipulation
3. Any malicious or unethical activity

